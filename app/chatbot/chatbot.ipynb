{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "import sys\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sqlite3\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"PC Builder\",\n",
    "  instructions=\"You are a knowledgeable technical assistant that helps the users build their personal computers. Your main task is to take the user’s expectations and requirements for their machine. \",\n",
    "  tools=[],\n",
    "  model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=\"I want to build a pc to play gta 5. Can you help me?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_oAf0GOQFmZrCBq931Dq6YiuC', assistant_id='asst_iILqB2gpG4KqHjQZReP3vLds', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Here's an image of a sample PC build that can run GTA 5 smoothly.\\n\\n![Sample PC Build](https://image-url.com)\\n\\nThis is a visual reference for the kind of setup you might be aiming for. Make sure your selected components fit this aesthetic and functionality, and happy building!\"), type='text')], created_at=1742043676, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_06b3fBUIZX3xsJnUQoKbb7iq', status=None, thread_id='thread_kpZZxa6D1qERWKOZPvdW7SD1'), Message(id='msg_VcDsVtR9KhBNHQhkGLrAEVZM', assistant_id='asst_iILqB2gpG4KqHjQZReP3vLds', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Of course, Jane Doe! I'd be happy to help you build a PC that can run GTA 5 smoothly. Here's a general guide to building a PC suitable for playing GTA 5 with good performance:\\n\\n### Essential Components:\\n\\n1. **Processor (CPU):**\\n   - **Recommendation:** AMD Ryzen 5 5600X or Intel Core i5-12400.\\n   - These processors provide excellent performance for gaming at a reasonable price.\\n\\n2. **Graphics Card (GPU):**\\n   - **Recommendation:** NVIDIA GeForce RTX 3060 or AMD Radeon RX 6600 XT.\\n   - These GPUs will allow you to enjoy GTA 5 at high settings and 1080p or even 1440p resolution.\\n\\n3. **Memory (RAM):**\\n   - **Recommendation:** 16 GB DDR4-3200.\\n   - This is more than adequate for gaming and ensures smooth multitasking.\\n\\n4. **Storage:**\\n   - **Recommendation:** 1 TB SSD (e.g., Samsung 970 EVO Plus).\\n   - An SSD will ensure faster load times compared to traditional hard drives.\\n\\n5. **Motherboard:**\\n   - **Recommendation:** Choose a motherboard compatible with your CPU, like the MSI B550 TOMAHAWK for AMD or ASUS PRIME B660-PLUS for Intel.\\n\\n6. **Power Supply (PSU):**\\n   - **Recommendation:** 650W 80 Plus Gold.\\n   - This provides enough power and efficiency for your system with room for future upgrades.\\n\\n7. **Case:**\\n   - **Recommendation:** Mid-Tower ATX case with good airflow.\\n   - Make sure it fits your components and has adequate cooling.\\n\\n8. **Cooling:**\\n   - **Recommendation:** Stock cooler bundled with the CPU should suffice, or you can opt for an aftermarket air cooler for better thermals.\\n   \\n9. **Operating System:**\\n   - **Recommendation:** Windows 10 or Windows 11.\\n   - Ensure you have a legitimate copy to run your games and software smoothly.\\n\\n### Optional Enhancements:\\n\\n- **Additional Storage:**\\n  - Consider adding a secondary HDD or another SSD for extra storage, especially if you plan to store more games.\\n\\n- **Aftermarket CPU Cooler:**\\n  - For quieter operation and better cooling, especially if overclocking.\\n\\n- **Additional Cooling Fans:**\\n  - To improve airflow if you plan on upgrading or overclocking in the future.\\n\\nBefore you start purchasing components:\\n- Consider checking for sales or bundle deals.\\n- Ensure all parts are compatible by using a tool like PCPartPicker.\\n- Double-check the size of your case to ensure all components fit.\\n\\nIf you have any more questions or need further assistance, feel free to ask. Enjoy building your PC and playing GTA 5, Jane Doe!\"), type='text')], created_at=1742043649, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_D7sM4GrkpG7AreCBprVMDGpW', status=None, thread_id='thread_kpZZxa6D1qERWKOZPvdW7SD1'), Message(id='msg_tnTvenA7CAca8T5xLF086S0M', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='I want to build a pc to play gta 5. Can you help me?'), type='text')], created_at=1742043545, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_kpZZxa6D1qERWKOZPvdW7SD1')], has_more=False, object='list', first_id='msg_oAf0GOQFmZrCBq931Dq6YiuC', last_id='msg_tnTvenA7CAca8T5xLF086S0M')\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n",
    ")\n",
    "\n",
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print(messages)\n",
    "else:\n",
    "  print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Retrieve your assistant (using your assistant ID)\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"PC Builder\",\n",
    "  instructions=\"You are a knowledgeable technical assistant that helps the users build their personal computers. Your main task is to take the user’s expectations and requirements for their machine and look for information online about the specific component requirements. \",\n",
    "  tools=[],\n",
    "  model=\"gpt-4o\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is running. Type 'exit' or 'quit' to end the session.\n",
      "\n",
      "\n",
      "assistant > Hello! It's great to meet you. I'm a knowledgeable technical assistant here to help you build your personal computer. Whether you're looking for specific performance requirements, budget constraints, or any particular features, I'm here to gather your requirements and recommend the best components for your new PC. What do you have in mind?"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Message content must be non-empty.', 'type': 'invalid_request_error', 'param': 'content', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m openai.beta.threads.runs.stream(\n\u001b[32m     68\u001b[39m             thread_id=thread.id,\n\u001b[32m     69\u001b[39m             assistant_id=assistant.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m             event_handler=MyEventHandler(),\n\u001b[32m     75\u001b[39m         ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m     76\u001b[39m             stream.until_done()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mrun_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mrun_chat\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Append the user's message to the thread\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_input\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Create a new streaming run for the updated conversation\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m openai.beta.threads.runs.stream(\n\u001b[32m     68\u001b[39m     thread_id=thread.id,\n\u001b[32m     69\u001b[39m     assistant_id=assistant.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     event_handler=MyEventHandler(),\n\u001b[32m     75\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/resources/beta/threads/messages.py:102\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, thread_id, content, role, attachments, metadata, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `thread_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthread_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mOpenAI-Beta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistants=v2\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/threads/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthread_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattachments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattachments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Message content must be non-empty.', 'type': 'invalid_request_error', 'param': 'content', 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Create a new conversation thread (once)\n",
    "thread = openai.beta.threads.create()\n",
    "\n",
    "# Retrieve your assistant by its ID using retrieve()\n",
    "assistant = openai.beta.assistants.retrieve(\"asst_N6pOdHNreq2yVJjJvR6JbNE9\")\n",
    "\n",
    "# Define an event handler to process streaming events\n",
    "class MyEventHandler(AssistantEventHandler):\n",
    "    def on_text_created(self, text) -> None:\n",
    "        # This is called when the assistant begins a new message\n",
    "        print(\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        # Called for each streaming token (delta)\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        # Handle tool call events if applicable\n",
    "        print(f\"\\nassistant (tool call: {tool_call.type})\", flush=True)\n",
    "  \n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "def run_initial_welcome():\n",
    "    # This run will trigger the assistant to send a welcome message\n",
    "    with openai.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=(\n",
    "            \"Please start the conversation by greeting the user. \"\n",
    "            \"Explain that you are a technical assistant who helps users build personal computers by gathering their requirements and providing component recommendations.\"\n",
    "        ),\n",
    "        event_handler=MyEventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "def run_chat():\n",
    "    print(\"Chatbot is running. Type 'exit' or 'quit' to end the session.\\n\")\n",
    "    \n",
    "    # First, run the initial welcome message from the assistant\n",
    "    run_initial_welcome()\n",
    "    \n",
    "    # Then, enter the interactive loop for user messages\n",
    "    while True:\n",
    "        user_input = input(\"\\nuser > \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Append the user's message to the thread\n",
    "        openai.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_input\n",
    "        )\n",
    "        \n",
    "        # Create a new streaming run for the updated conversation\n",
    "        with openai.beta.threads.runs.stream(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            instructions=(\n",
    "                \"You are a knowledgeable technical assistant that helps users build personal computers. \"\n",
    "                \"Gather the user's requirements and provide helpful recommendations.\"\n",
    "            ),\n",
    "            event_handler=MyEventHandler(),\n",
    "        ) as stream:\n",
    "            stream.until_done()\n",
    "\n",
    "def sql_query_tool(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against the components database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"components.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        if rows:\n",
    "            # Format the result rows\n",
    "            return \"\\n\".join(str(row) for row in rows)\n",
    "        else:\n",
    "            return \"No matching components found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You type the query yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is running. Type 'exit' or 'quit' to end the session.\n",
      "\n",
      "\n",
      "assistant > Hello! I'm glad to see you here. I'm a knowledgeable technical assistant, and I'm here to help you build your personal computer. Whether you're looking for recommendations based on your specific needs, budget, or performance expectations, just let me know your requirements, and I'll provide tailored component suggestions. What can I assist you with today?"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Message content must be non-empty.', 'type': 'invalid_request_error', 'param': 'content', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m openai.beta.threads.runs.stream(\n\u001b[32m    134\u001b[39m             thread_id=thread.id,\n\u001b[32m    135\u001b[39m             assistant_id=assistant.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m             event_handler=MyEventHandler(),\n\u001b[32m    141\u001b[39m         ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m    142\u001b[39m             stream.until_done()\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mrun_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mrun_chat\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Otherwise, treat it as a normal chat message:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_input\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Create a new streaming run for each user message\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m openai.beta.threads.runs.stream(\n\u001b[32m    134\u001b[39m     thread_id=thread.id,\n\u001b[32m    135\u001b[39m     assistant_id=assistant.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m     event_handler=MyEventHandler(),\n\u001b[32m    141\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/resources/beta/threads/messages.py:102\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, thread_id, content, role, attachments, metadata, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `thread_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthread_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mOpenAI-Beta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistants=v2\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/threads/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthread_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattachments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattachments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/kinderishacking/BeEpBoOp/chatbot/venv/lib/python3.12/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Message content must be non-empty.', 'type': 'invalid_request_error', 'param': 'content', 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Create a new conversation thread (once)\n",
    "thread = openai.beta.threads.create()\n",
    "\n",
    "# Retrieve your assistant by its ID using retrieve()\n",
    "assistant = openai.beta.assistants.retrieve(\"asst_N6pOdHNreq2yVJjJvR6JbNE9\")\n",
    "\n",
    "# ---------------------\n",
    "# Define the Streaming Event Handler\n",
    "# ---------------------\n",
    "class MyEventHandler(AssistantEventHandler):\n",
    "    def on_text_created(self, text) -> None:\n",
    "        # Called when the assistant starts a new message\n",
    "        print(\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        # Called for each token as it's streamed\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant (tool call: {tool_call.type})\", flush=True)\n",
    "  \n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "# ---------------------\n",
    "# Chatbot Functions\n",
    "# ---------------------\n",
    "def run_initial_welcome():\n",
    "    # This run triggers the assistant to send a welcome message\n",
    "    with openai.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=(\n",
    "            \"Please start the conversation by greeting the user. \"\n",
    "            \"Explain that you are a knowledgeable technical assistant who helps users build personal computers \"\n",
    "            \"by gathering their requirements and providing component recommendations.\"\n",
    "        ),\n",
    "        event_handler=MyEventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "# ---------------------\n",
    "# LangChain SQL Query Integration\n",
    "# ---------------------\n",
    "def sql_query_tool(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against the components database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"components.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        if rows:\n",
    "            return \"\\n\".join(str(row) for row in rows)\n",
    "        else:\n",
    "            return \"No matching components found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n",
    "\n",
    "# Wrap the SQL query tool as a LangChain Tool\n",
    "sql_tool = Tool(\n",
    "    name=\"SQLQuery\",\n",
    "    func=sql_query_tool,\n",
    "    description=(\n",
    "        \"Use this tool to query the SQL database for PC components from a table called 'components'. \"\n",
    "        \"The table has columns such as 'type', 'model', and 'specs'. \"\n",
    "        \"Input should be a valid SQL SELECT statement.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize a LangChain Agent with the SQL tool\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent([sql_tool], llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "def run_sql_query(requirements: str) -> str:\n",
    "    \"\"\"\n",
    "    Use the LangChain agent to interpret the user requirements and generate a SQL query.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Based on the following requirement, generate a SQL query that selects all matching PC components \"\n",
    "        \"from a table called 'components'. The table has columns such as 'type', 'model', and 'specs'. \"\n",
    "        f\"Requirement: {requirements}\"\n",
    "    )\n",
    "    sql_query = agent.run(prompt)\n",
    "    return sql_query\n",
    "\n",
    "# ---------------------\n",
    "# Main Chat Loop with SQL Query Integration\n",
    "# ---------------------\n",
    "def run_chat():\n",
    "    print(\"Chatbot is running. Type 'exit' or 'quit' to end the session.\\n\")\n",
    "    \n",
    "    # First, display the welcome message from the assistant\n",
    "    run_initial_welcome()\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nuser > \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # If the user wants to query the database, use a special command\n",
    "        if user_input.strip().lower().startswith(\"query:\"):\n",
    "            # Extract requirement text from after \"query:\"\n",
    "            requirements = user_input.strip()[len(\"query:\"):].strip()\n",
    "            print(\"\\nassistant (processing SQL query)...\")\n",
    "            generated_query = run_sql_query(requirements)\n",
    "            print(\"\\nassistant (SQL Query):\", generated_query)\n",
    "            query_result = sql_query_tool(generated_query)\n",
    "            print(\"\\nassistant (SQL Result):\", query_result)\n",
    "            # Append the SQL query and result to the conversation thread\n",
    "            openai.beta.threads.messages.create(\n",
    "                thread_id=thread.id,\n",
    "                role=\"assistant\",\n",
    "                content=f\"SQL Query: {generated_query}\\nResults:\\n{query_result}\"\n",
    "            )\n",
    "            continue\n",
    "        \n",
    "        # Otherwise, treat it as a normal chat message:\n",
    "        openai.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_input\n",
    "        )\n",
    "        \n",
    "        # Create a new streaming run for each user message\n",
    "        with openai.beta.threads.runs.stream(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            instructions=(\n",
    "                \"You are a knowledgeable technical assistant that helps users build personal computers. \"\n",
    "                \"Gather the user's requirements and provide helpful recommendations.\"\n",
    "            ),\n",
    "            event_handler=MyEventHandler(),\n",
    "        ) as stream:\n",
    "            stream.until_done()\n",
    "\n",
    "run_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bot where \"query:\" calls the last assistant message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is running. Type 'exit' or 'quit' to end the session.\n",
      "\n",
      "\n",
      "assistant > Hello! I'm Tecco, your technical assistant for building personal computers. I'm here to help you gather your requirements and provide tailored component recommendations to create the perfect PC for your needs. Whether you're looking for a gaming rig, a workstation, or something for everyday tasks, feel free to share your preferences or questions!"
     ]
    }
   ],
   "source": [
    "# Create a new conversation thread (once)\n",
    "thread = openai.beta.threads.create()\n",
    "\n",
    "# Retrieve your assistant by its ID using retrieve()\n",
    "assistant = openai.beta.assistants.retrieve(\"asst_N6pOdHNreq2yVJjJvR6JbNE9\")\n",
    "\n",
    "# ---------------------\n",
    "# Define the Streaming Event Handler\n",
    "# ---------------------\n",
    "class MyEventHandler(AssistantEventHandler):\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant (tool call: {tool_call.type})\", flush=True)\n",
    "  \n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "# ---------------------\n",
    "# Chatbot Functions\n",
    "# ---------------------\n",
    "def run_initial_welcome():\n",
    "    with openai.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=(\n",
    "            \"Please start the conversation by greeting the user. \"\n",
    "            \"Introduce yourself as Tecco and explain that you are a technical assistant who helps users build personal computers \"\n",
    "            \"by providing component recommendations.\"\n",
    "        ),\n",
    "        event_handler=MyEventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "def get_last_assistant_message() -> str:\n",
    "    \"\"\"Retrieve the content of the latest assistant message from the thread.\"\"\"\n",
    "    messages = openai.beta.threads.messages.list(thread_id=thread.id)\n",
    "    assistant_msgs = [msg for msg in messages if msg.role == \"assistant\"]\n",
    "    if assistant_msgs:\n",
    "        return assistant_msgs[-1].content or \"\"\n",
    "    return \"\"\n",
    "\n",
    "# ---------------------\n",
    "# LangChain SQL Query Integration\n",
    "# ---------------------\n",
    "def sql_query_tool(query: str) -> str:\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"components.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        if rows:\n",
    "            return \"\\n\".join(str(row) for row in rows)\n",
    "        else:\n",
    "            return \"No matching components found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {e}\"\n",
    "\n",
    "# Wrap the SQL query tool as a LangChain Tool\n",
    "\n",
    "sql_tool = Tool(\n",
    "    name=\"SQLQuery\",\n",
    "    func=sql_query_tool,\n",
    "    description=(\n",
    "        \"Use this tool to query the SQL database for PC components from a table called 'components'. \"\n",
    "        \"The table has columns such as 'type', 'model', and 'specs'. \"\n",
    "        \"Input should be a valid SQL SELECT statement.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent([sql_tool], llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "def run_sql_query(requirements: str) -> str:\n",
    "    prompt = (\n",
    "        \"Based on the following suggested components, generate a SQL query that selects all matching PC components \"\n",
    "        \"from a table called 'components'. The table has columns such as 'type', 'model', and 'specs'. \"\n",
    "        f\"Components: {requirements}\"\n",
    "    )\n",
    "    sql_query = agent.run(prompt)\n",
    "    return sql_query\n",
    "\n",
    "# ---------------------\n",
    "# Main Chat Loop with SQL Query Integration\n",
    "# ---------------------\n",
    "def run_chat():\n",
    "    print(\"Chatbot is running. Type 'exit' or 'quit' to end the session.\\n\")\n",
    "    \n",
    "    # Display the welcome message first.\n",
    "    run_initial_welcome()\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nuser > \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # If the user input starts with \"query:\" then use that for the SQL query.\n",
    "        if user_input.strip().lower().startswith(\"query:\"):\n",
    "            # Extract any additional text provided after \"query:\"\n",
    "            extra = user_input.strip()[len(\"query:\"):].strip()\n",
    "            if extra:\n",
    "                requirements = extra\n",
    "            else:\n",
    "                # If no extra text is provided, use the last assistant message.\n",
    "                requirements = get_last_assistant_message()\n",
    "            print(\"\\nassistant (processing SQL query)...\")\n",
    "            generated_query = run_sql_query(requirements)\n",
    "            print(\"\\nassistant (SQL Query):\", generated_query)\n",
    "            query_result = sql_query_tool(generated_query)\n",
    "            print(\"\\nassistant (SQL Result):\", query_result)\n",
    "            # Append the SQL query and result as an assistant message to the conversation thread.\n",
    "            openai.beta.threads.messages.create(\n",
    "                thread_id=thread.id,\n",
    "                role=\"assistant\",\n",
    "                content=f\"SQL Query: {generated_query}\\nResults:\\n{query_result}\"\n",
    "            )\n",
    "            continue\n",
    "        \n",
    "        # Otherwise, treat it as a normal chat message:\n",
    "        openai.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_input\n",
    "        )\n",
    "        \n",
    "        with openai.beta.threads.runs.stream(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            instructions=(\n",
    "                \"You are a knowledgeable technical assistant that helps users build personal computers. \"\n",
    "                \"Gather the user's requirements and provide helpful recommendations.\"\n",
    "            ),\n",
    "            event_handler=MyEventHandler(),\n",
    "        ) as stream:\n",
    "            stream.until_done()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
